{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, root_mean_squared_error, explained_variance_score, max_error, d2_absolute_error_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from surprise import SVD, Reader, Dataset, SVDpp, accuracy\n",
    "from collections import defaultdict\n",
    "import gzip\n",
    "from surprise.model_selection import cross_validate, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Ideas:\n",
    "- use the BPE tokens and create word embeddings from them\n",
    "    - using Word2Vec or using pretrained which will use the raw reviews however\n",
    "    - this means we can create sentiment analysis based on the food itself\n",
    "- SVDpp makes latent features that are put into LightGBM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('interactions_train.csv')\n",
    "df_test = pd.read_csv('interactions_test.csv')\n",
    "df_validation = pd.read_csv('interactions_validation.csv')\n",
    "pp_recipes = pd.read_csv('PP_recipes.csv')\n",
    "pp_users = pd.read_csv('PP_users.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat(df):\n",
    "    df['name_embds'] = df['name_tokens'].apply(lambda x: len(eval(x)) if isinstance(x, str) else 0)\n",
    "    df['ingredient_embds'] = df['ingredient_tokens'].apply(lambda x: len(eval(x)) if isinstance(x, str) else 0)\n",
    "    df['steps_length'] = df['steps_tokens'].apply(lambda x: len(eval(x)) if isinstance(x, str) else 0)\n",
    "    return df\n",
    "\n",
    "def add_avg_user_rating(df, user_avg_rating):\n",
    "    df = df.merge(user_avg_rating, on='u', how='left')\n",
    "    df['avg_rating'].fillna(df_train['rating'].mean(), inplace=True)\n",
    "    return df\n",
    "\n",
    "user_avg_rating = df_train.groupby('u')['rating'].mean().reset_index()\n",
    "user_avg_rating.columns = ['u', 'avg_rating']\n",
    "\n",
    "df_train_enriched = df_train.merge(pp_recipes, left_on='i', right_on='id')\n",
    "df_train_enriched = df_train_enriched.merge(pp_users, left_on='u', right_on='u')\n",
    "\n",
    "df_valid_enriched = df_validation.merge(pp_recipes, left_on='i', right_on='id')\n",
    "df_valid_enriched = df_valid_enriched.merge(pp_users, left_on='u', right_on='u')\n",
    "\n",
    "df_train_enriched = add_avg_user_rating(df_train_enriched, user_avg_rating)\n",
    "df_valid_enriched = add_avg_user_rating(df_valid_enriched, user_avg_rating)\n",
    "\n",
    "df_train_enriched = feat(df_train_enriched)\n",
    "df_valid_enriched = feat(df_valid_enriched)\n",
    "\n",
    "df_train_svd = df_train_enriched[['u', 'id', 'rating']]\n",
    "df_valid_svd = df_valid_enriched[['u', 'id', 'rating']]\n",
    "\n",
    "df_train_enriched.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(0, 5))\n",
    "data = Dataset.load_from_df(df_train_svd, reader)\n",
    "\n",
    "trainset = data.build_full_trainset()\n",
    "\n",
    "best_params = {\n",
    "    'n_factors': 150,\n",
    "    'lr_all': 0.005,\n",
    "    'reg_all': 0.1,\n",
    "    'n_epochs': 10,\n",
    "    \"verbose\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = SVDpp(**best_params)\n",
    "algo.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = Dataset.load_from_df(df_valid_svd, reader)\n",
    "validset = valid_data.build_full_trainset().build_testset()\n",
    "\n",
    "predictions = algo.test(validset)\n",
    "\n",
    "rmse = accuracy.rmse(predictions, verbose=True)\n",
    "\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_latent_features(model, df, user_col, item_col):\n",
    "#     user_features = {}\n",
    "#     item_features = {}\n",
    "\n",
    "#     for user_id in df[user_col].unique():\n",
    "#         user_features[user_id] = model.pu[model.trainset.to_inner_uid(user_id)]\n",
    "#     for item_id in df[item_col].unique():\n",
    "#         item_features[item_id] = model.qi[model.trainset.to_inner_iid(item_id)]\n",
    "\n",
    "#     return user_features, item_features\n",
    "\n",
    "# user_features, item_features = extract_latent_features(algo, df_train_enriched, 'u', 'id')\n",
    "\n",
    "# def add_latent_features(df, user_features, item_features):\n",
    "#     df['user_latent'] = df['u'].apply(lambda x: user_features.get(x, np.zeros(len(next(iter(user_features.values()))))))\n",
    "#     df['item_latent'] = df['id'].apply(lambda x: item_features.get(x, np.zeros(len(next(iter(item_features.values()))))))\n",
    "#     return df\n",
    "\n",
    "# df_train_enriched = add_latent_features(df_train_enriched, user_features, item_features)\n",
    "# df_valid_enriched = add_latent_features(df_valid_enriched, user_features, item_features)\n",
    "\n",
    "# def expand_latent_features(df):\n",
    "#     latent_dim = len(df['user_latent'][0])  # Assume user and item latent dimensions are the same\n",
    "#     for i in range(latent_dim):\n",
    "#         df[f'user_latent_{i}'] = df['user_latent'].apply(lambda x: x[i])\n",
    "#         df[f'item_latent_{i}'] = df['item_latent'].apply(lambda x: x[i])\n",
    "#     return df.drop(columns=['user_latent', 'item_latent'])\n",
    "\n",
    "# df_train_enriched = expand_latent_features(df_train_enriched)\n",
    "# df_valid_enriched = expand_latent_features(df_valid_enriched)\n",
    "\n",
    "def predict_svd_ratings(model, df):\n",
    "    predictions = []\n",
    "    for _, row in df.iterrows():\n",
    "        user = row['u']\n",
    "        item = row['id']\n",
    "        try:\n",
    "            pred = model.predict(user, item).est\n",
    "        except ValueError:\n",
    "            pred = model.trainset.global_mean\n",
    "        predictions.append(pred)\n",
    "    df['latent_rating'] = predictions\n",
    "    return df\n",
    "\n",
    "df_train_enriched = predict_svd_ratings(algo, df_train_enriched)\n",
    "df_valid_enriched = predict_svd_ratings(algo, df_valid_enriched)\n",
    "\n",
    "df_train_enriched.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import ast\n",
    "\n",
    "# Example preprocessing\n",
    "def pad_sequences(sequences, maxlen, padding='post', value=0):\n",
    "    padded = np.full((len(sequences), maxlen), fill_value=value)\n",
    "    for idx, seq in enumerate(sequences):\n",
    "        seq = ast.literal_eval(seq)\n",
    "        print(seq)\n",
    "        padded[idx, :min(len(seq), maxlen)] = seq[:maxlen]\n",
    "    return padded\n",
    "\n",
    "# Pad name_tokens, ingredient_tokens, and steps_tokens\n",
    "max_name_len = 10\n",
    "max_steps_len = 50\n",
    "max_ingredient_len = 20  # Per ingredient\n",
    "max_ingredients = 10     # Number of ingredients\n",
    "\n",
    "# Prepare padded sequences\n",
    "pp_recipes['name_tokens_padded'] = pad_sequences(pp_recipes['name_tokens'].tolist(), max_name_len).tolist()\n",
    "pp_recipes['steps_tokens_padded'] = pad_sequences(pp_recipes['steps_tokens'].tolist(), max_steps_len).tolist()\n",
    "\n",
    "# Convert data into PyTorch-friendly format\n",
    "class RecipeDataset(Dataset):\n",
    "    def __init__(self, recipes):\n",
    "        self.name_tokens = torch.tensor(np.array(list(recipes['name_tokens_padded'])), dtype=torch.long)\n",
    "        self.steps_tokens = torch.tensor(np.array(list(recipes['steps_tokens_padded'])), dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.name_tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'name_tokens': self.name_tokens[idx],\n",
    "            'steps_tokens': self.steps_tokens[idx],\n",
    "        }\n",
    "\n",
    "class RecipeEmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(RecipeEmbeddingModel, self).__init__()\n",
    "        # Embedding layers\n",
    "        self.name_embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.steps_embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * 3, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, name_tokens, steps_tokens, ingredient_tokens):\n",
    "        # Embedding lookup\n",
    "        name_embeds = self.name_embedding(name_tokens).mean(dim=1)\n",
    "        steps_embeds = self.steps_embedding(steps_tokens).mean(dim=1)\n",
    "        combined = torch.cat([name_embeds, steps_embeds], dim=1)\n",
    "        output = self.fc(combined)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RecipeDataset(pp_recipes)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "vocab_size = 50000\n",
    "embedding_dim = 128\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Model, optimizer, loss\n",
    "model = RecipeEmbeddingModel(vocab_size, embedding_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in dataloader:\n",
    "        name_tokens = batch['name_tokens']\n",
    "        steps_tokens = batch['steps_tokens']\n",
    "\n",
    "        outputs = model(name_tokens, steps_tokens)\n",
    "\n",
    "        targets = torch.tensor(df_train['rating'].values, dtype=torch.float32)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload embedding layers\n",
    "name_embedding = model.name_embedding\n",
    "steps_embedding = model.steps_embedding\n",
    "# ingredient_embedding = model.ingredient_embedding\n",
    "\n",
    "# Set embeddings to evaluation mode (freeze weights)\n",
    "name_embedding.eval()\n",
    "steps_embedding.eval()\n",
    "# ingredient_embedding.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute embedding features\n",
    "def compute_embeddings(recipe, name_emb_layer, steps_emb_layer, ingredient_emb_layer):\n",
    "    # Convert tokens to tensors\n",
    "    name_tokens = torch.tensor(recipe['name_tokens_padded'], dtype=torch.long)\n",
    "    steps_tokens = torch.tensor(recipe['steps_tokens_padded'], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        name_emb = name_emb_layer(name_tokens).mean(dim=0).numpy()\n",
    "        steps_emb = steps_emb_layer(steps_tokens).mean(dim=0).numpy()\n",
    "\n",
    "    return np.concatenate([name_emb, steps_emb])\n",
    "\n",
    "\n",
    "# Apply to all recipes\n",
    "embedding_features = pp_recipes.apply(\n",
    "    lambda x: compute_embeddings(\n",
    "        # x, name_embedding, steps_embedding, ingredient_embedding\n",
    "        x, name_embedding, steps_embedding\n",
    "\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "embedding_features_df = pd.DataFrame(embedding_features.tolist())\n",
    "embedding_features_df.columns = [f'embedding_feature_{i}' for i in range(embedding_features_df.shape[1])]\n",
    "\n",
    "pp_recipes = pd.concat([pp_recipes, embedding_features_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_recipe_embeddings(df, recipe_embeddings):\n",
    "    return df.merge(recipe_embeddings, left_on='i', right_on='id', how='left')\n",
    "\n",
    "df_train_enriched = add_recipe_embeddings(df_train_enriched, pp_recipes)\n",
    "df_valid_enriched = add_recipe_embeddings(df_valid_enriched, pp_recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "features = ['u','id','name_embds', 'ingredient_embds', 'steps_length', 'calorie_level', 'avg_rating']\n",
    "latent_features = [col for col in df_train_enriched.columns if 'latent' in col]\n",
    "features = features \\\n",
    "        + latent_features \\\n",
    "        # + embedding_feature_cols\n",
    "target = 'rating'\n",
    "\n",
    "X_train = df_train_enriched[features].astype(\"int\")\n",
    "y_train = df_train_enriched[target].astype(\"int\")\n",
    "X_val = df_valid_enriched[features].astype(\"int\")\n",
    "y_val = df_valid_enriched[target].astype(\"int\")\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "\n",
    "# Set LightGBM parameters\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.01,\n",
    "    'num_leaves': 63,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "# Train model\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[train_data, val_data],\n",
    "    num_boost_round=3000,\n",
    ")\n",
    "y_val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "\n",
    "rmse_val = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "\n",
    "print(f\"Validation RMSE: {rmse_val:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
