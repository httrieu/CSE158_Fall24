{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, root_mean_squared_error, explained_variance_score, max_error, d2_absolute_error_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from surprise import SVD, Reader, Dataset, SVDpp, accuracy\n",
    "from collections import defaultdict\n",
    "import gzip\n",
    "from surprise.model_selection import cross_validate, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Ideas:\n",
    "- use the BPE tokens and create word embeddings from them\n",
    "    - using Word2Vec or using pretrained which will use the raw reviews however\n",
    "    - this means we can create sentiment analysis based on the food itself\n",
    "- SVDpp makes latent features that are put into LightGBM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('interactions_train.csv')\n",
    "df_test = pd.read_csv('interactions_test.csv')\n",
    "df_validation = pd.read_csv('interactions_validation.csv')\n",
    "pp_recipes = pd.read_csv('PP_recipes.csv')\n",
    "pp_users = pd.read_csv('PP_users.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>recipe_id</th>\n",
       "      <th>date</th>\n",
       "      <th>rating</th>\n",
       "      <th>u</th>\n",
       "      <th>i</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2046</td>\n",
       "      <td>4684</td>\n",
       "      <td>2000-02-25</td>\n",
       "      <td>5.0</td>\n",
       "      <td>22095</td>\n",
       "      <td>44367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2046</td>\n",
       "      <td>517</td>\n",
       "      <td>2000-02-25</td>\n",
       "      <td>5.0</td>\n",
       "      <td>22095</td>\n",
       "      <td>87844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1773</td>\n",
       "      <td>7435</td>\n",
       "      <td>2000-03-13</td>\n",
       "      <td>5.0</td>\n",
       "      <td>24732</td>\n",
       "      <td>138181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1773</td>\n",
       "      <td>278</td>\n",
       "      <td>2000-03-13</td>\n",
       "      <td>4.0</td>\n",
       "      <td>24732</td>\n",
       "      <td>93054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2046</td>\n",
       "      <td>3431</td>\n",
       "      <td>2000-04-07</td>\n",
       "      <td>5.0</td>\n",
       "      <td>22095</td>\n",
       "      <td>101723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  recipe_id        date  rating      u       i\n",
       "0     2046       4684  2000-02-25     5.0  22095   44367\n",
       "1     2046        517  2000-02-25     5.0  22095   87844\n",
       "2     1773       7435  2000-03-13     5.0  24732  138181\n",
       "3     1773        278  2000-03-13     4.0  24732   93054\n",
       "4     2046       3431  2000-04-07     5.0  22095  101723"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1m/tdk2ds5571l4_8krg7f3c7x00000gn/T/ipykernel_59861/1468149453.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['avg_rating'].fillna(df_train['rating'].mean(), inplace=True)\n",
      "/var/folders/1m/tdk2ds5571l4_8krg7f3c7x00000gn/T/ipykernel_59861/1468149453.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['avg_rating'].fillna(df_train['rating'].mean(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "def feat(df):\n",
    "    df['name_embds'] = df['name_tokens'].apply(lambda x: len(eval(x)) if isinstance(x, str) else 0)\n",
    "    df['ingredient_embds'] = df['ingredient_tokens'].apply(lambda x: len(eval(x)) if isinstance(x, str) else 0)\n",
    "    df['steps_length'] = df['steps_tokens'].apply(lambda x: len(eval(x)) if isinstance(x, str) else 0)\n",
    "    return df\n",
    "\n",
    "def add_avg_user_rating(df, user_avg_rating):\n",
    "    df = df.merge(user_avg_rating, on='u', how='left')\n",
    "    df['avg_rating'].fillna(df_train['rating'].mean(), inplace=True)\n",
    "    return df\n",
    "\n",
    "user_avg_rating = df_train.groupby('u')['rating'].mean().reset_index()\n",
    "user_avg_rating.columns = ['u', 'avg_rating']\n",
    "\n",
    "df_train_enriched = df_train.merge(pp_recipes, left_on='i', right_on='id')\n",
    "df_train_enriched = df_train_enriched.merge(pp_users, left_on='u', right_on='u')\n",
    "\n",
    "df_valid_enriched = df_validation.merge(pp_recipes, left_on='i', right_on='id')\n",
    "df_valid_enriched = df_valid_enriched.merge(pp_users, left_on='u', right_on='u')\n",
    "\n",
    "df_train_enriched = add_avg_user_rating(df_train_enriched, user_avg_rating)\n",
    "df_valid_enriched = add_avg_user_rating(df_valid_enriched, user_avg_rating)\n",
    "\n",
    "df_train_enriched = feat(df_train_enriched)\n",
    "df_valid_enriched = feat(df_valid_enriched)\n",
    "\n",
    "df_train_svd = df_train_enriched[['u', 'id', 'rating']]\n",
    "df_valid_svd = df_valid_enriched[['u', 'id', 'rating']]\n",
    "\n",
    "df_train_enriched.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(0, 5))\n",
    "data = Dataset.load_from_df(df_train_svd, reader)\n",
    "\n",
    "trainset = data.build_full_trainset()\n",
    "\n",
    "best_params = {\n",
    "    'n_factors': 100,\n",
    "    'lr_all': 0.005,\n",
    "    'reg_bu': 0.3,\n",
    "    'reg_yj': 0.02,\n",
    "    'reg_bi': 0.03,\n",
    "    'reg_pu': 0.6,\n",
    "    'reg_qi': 0.4,\n",
    "    'n_epochs': 150,\n",
    "    \"verbose\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing epoch 0\n",
      " processing epoch 1\n",
      " processing epoch 2\n",
      " processing epoch 3\n",
      " processing epoch 4\n",
      " processing epoch 5\n",
      " processing epoch 6\n",
      " processing epoch 7\n",
      " processing epoch 8\n",
      " processing epoch 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVDpp at 0x17b9e9370>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo = SVDpp(**best_params)\n",
    "algo.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.3065\n",
      "RMSE: 1.306490910166436\n"
     ]
    }
   ],
   "source": [
    "valid_data = Dataset.load_from_df(df_valid_svd, reader)\n",
    "validset = valid_data.build_full_trainset().build_testset()\n",
    "\n",
    "predictions = algo.test(validset)\n",
    "\n",
    "rmse = accuracy.rmse(predictions, verbose=True)\n",
    "\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1m/tdk2ds5571l4_8krg7f3c7x00000gn/T/ipykernel_59861/3344921843.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['user_latent'] = df['u'].apply(lambda x: user_features.get(x, np.zeros(len(next(iter(user_features.values()))))))\n",
      "/var/folders/1m/tdk2ds5571l4_8krg7f3c7x00000gn/T/ipykernel_59861/3344921843.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['item_latent'] = df['id'].apply(lambda x: item_features.get(x, np.zeros(len(next(iter(item_features.values()))))))\n",
      "/var/folders/1m/tdk2ds5571l4_8krg7f3c7x00000gn/T/ipykernel_59861/3344921843.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['user_latent'] = df['u'].apply(lambda x: user_features.get(x, np.zeros(len(next(iter(user_features.values()))))))\n",
      "/var/folders/1m/tdk2ds5571l4_8krg7f3c7x00000gn/T/ipykernel_59861/3344921843.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['item_latent'] = df['id'].apply(lambda x: item_features.get(x, np.zeros(len(next(iter(item_features.values()))))))\n"
     ]
    }
   ],
   "source": [
    "def extract_latent_features(model, df, user_col, item_col):\n",
    "    user_features = {}\n",
    "    item_features = {}\n",
    "\n",
    "    for user_id in df[user_col].unique():\n",
    "        user_features[user_id] = model.pu[model.trainset.to_inner_uid(user_id)]\n",
    "    for item_id in df[item_col].unique():\n",
    "        item_features[item_id] = model.qi[model.trainset.to_inner_iid(item_id)]\n",
    "\n",
    "    return user_features, item_features\n",
    "\n",
    "user_features, item_features = extract_latent_features(algo, df_train_enriched, 'u', 'id')\n",
    "\n",
    "def add_latent_features(df, user_features, item_features):\n",
    "    df['user_latent'] = df['u'].apply(lambda x: user_features.get(x, np.zeros(len(next(iter(user_features.values()))))))\n",
    "    df['item_latent'] = df['id'].apply(lambda x: item_features.get(x, np.zeros(len(next(iter(item_features.values()))))))\n",
    "    return df\n",
    "\n",
    "df_train_enriched = add_latent_features(df_train_enriched, user_features, item_features)\n",
    "df_valid_enriched = add_latent_features(df_valid_enriched, user_features, item_features)\n",
    "\n",
    "# Combine latent features with metadata for LightGBM\n",
    "def expand_latent_features(df):\n",
    "    latent_dim = len(df['user_latent'][0])  # Assume user and item latent dimensions are the same\n",
    "    for i in range(latent_dim):\n",
    "        df[f'user_latent_{i}'] = df['user_latent'].apply(lambda x: x[i])\n",
    "        df[f'item_latent_{i}'] = df['item_latent'].apply(lambda x: x[i])\n",
    "    return df.drop(columns=['user_latent', 'item_latent'])\n",
    "\n",
    "df_train_enriched = expand_latent_features(df_train_enriched)\n",
    "df_valid_enriched = expand_latent_features(df_valid_enriched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001637 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 796\n",
      "[LightGBM] [Info] Number of data points in the train set: 319572, number of used features: 7\n",
      "[LightGBM] [Info] Start training from score 4.573426\n",
      "Validation RMSE: 1.3055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sraswan/Documents/Workspace/CSE158_Fall24/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "features = ['u','id','name_embds', 'ingredient_embds', 'steps_length', 'calorie_level', 'avg_rating']\n",
    "features = features + [col for col in df_train_enriched.columns if 'latent' in col]\n",
    "target = 'rating'\n",
    "\n",
    "X_train = df_train_enriched[features].astype(\"int\")\n",
    "y_train = df_train_enriched[target].astype(\"int\")\n",
    "X_val = df_valid_enriched[features].astype(\"int\")\n",
    "y_val = df_valid_enriched[target].astype(\"int\")\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "\n",
    "# Set LightGBM parameters\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.01,\n",
    "    'num_leaves': 100,\n",
    "    'feature_fraction': 0.5,\n",
    "    'bagging_fraction': 0.5,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "# Train model\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[train_data, val_data],\n",
    "    num_boost_round=1000,\n",
    ")\n",
    "y_val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "\n",
    "rmse_val = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "\n",
    "print(f\"Validation RMSE: {rmse_val:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
