{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, root_mean_squared_error, explained_variance_score, max_error, d2_absolute_error_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from surprise import SVD, Reader, Dataset\n",
    "from collections import defaultdict\n",
    "import gzip\n",
    "from surprise import SVDpp\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name_tokens</th>\n",
       "      <th>ingredient_tokens</th>\n",
       "      <th>steps_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>424415</td>\n",
       "      <td>[40480, 37229, 2911, 1019, 249, 6878, 6878, 28...</td>\n",
       "      <td>[[2911, 1019, 249, 6878], [1353], [6953], [153...</td>\n",
       "      <td>[40480, 40482, 21662, 481, 6878, 500, 246, 161...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>146223</td>\n",
       "      <td>[40480, 18376, 7056, 246, 1531, 2032, 40481]</td>\n",
       "      <td>[[17918], [25916], [2507, 6444], [8467, 1179],...</td>\n",
       "      <td>[40480, 40482, 729, 2525, 10906, 485, 43, 8393...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312329</td>\n",
       "      <td>[40480, 21044, 16954, 8294, 556, 10837, 40481]</td>\n",
       "      <td>[[5867, 24176], [1353], [6953], [1301, 11332],...</td>\n",
       "      <td>[40480, 40482, 8240, 481, 24176, 296, 1353, 66...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74301</td>\n",
       "      <td>[40480, 10025, 31156, 40481]</td>\n",
       "      <td>[[1270, 1645, 28447], [21601], [27952, 29471, ...</td>\n",
       "      <td>[40480, 40482, 5539, 21601, 1073, 903, 2324, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>76272</td>\n",
       "      <td>[40480, 17841, 252, 782, 2373, 1641, 2373, 252...</td>\n",
       "      <td>[[1430, 11434], [1430, 17027], [1615, 23, 695,...</td>\n",
       "      <td>[40480, 40482, 14046, 1430, 11434, 488, 17027,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                        name_tokens  \\\n",
       "0  424415  [40480, 37229, 2911, 1019, 249, 6878, 6878, 28...   \n",
       "1  146223       [40480, 18376, 7056, 246, 1531, 2032, 40481]   \n",
       "2  312329     [40480, 21044, 16954, 8294, 556, 10837, 40481]   \n",
       "3   74301                       [40480, 10025, 31156, 40481]   \n",
       "4   76272  [40480, 17841, 252, 782, 2373, 1641, 2373, 252...   \n",
       "\n",
       "                                   ingredient_tokens  \\\n",
       "0  [[2911, 1019, 249, 6878], [1353], [6953], [153...   \n",
       "1  [[17918], [25916], [2507, 6444], [8467, 1179],...   \n",
       "2  [[5867, 24176], [1353], [6953], [1301, 11332],...   \n",
       "3  [[1270, 1645, 28447], [21601], [27952, 29471, ...   \n",
       "4  [[1430, 11434], [1430, 17027], [1615, 23, 695,...   \n",
       "\n",
       "                                        steps_tokens  \n",
       "0  [40480, 40482, 21662, 481, 6878, 500, 246, 161...  \n",
       "1  [40480, 40482, 729, 2525, 10906, 485, 43, 8393...  \n",
       "2  [40480, 40482, 8240, 481, 24176, 296, 1353, 66...  \n",
       "3  [40480, 40482, 5539, 21601, 1073, 903, 2324, 4...  \n",
       "4  [40480, 40482, 14046, 1430, 11434, 488, 17027,...  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe = pd.read_csv(\"PP_recipes.csv\")\n",
    "recipe = recipe[[\"id\", \"name_tokens\", \"ingredient_tokens\", \"steps_tokens\"]]\n",
    "recipe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>techniques</th>\n",
       "      <th>items</th>\n",
       "      <th>n_items</th>\n",
       "      <th>ratings</th>\n",
       "      <th>n_ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[8, 0, 0, 5, 6, 0, 0, 1, 0, 9, 1, 0, 0, 0, 1, ...</td>\n",
       "      <td>[1118, 27680, 32541, 137353, 16428, 28815, 658...</td>\n",
       "      <td>31</td>\n",
       "      <td>[5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 4.0, 4.0, ...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[11, 0, 0, 2, 12, 0, 0, 0, 0, 14, 5, 0, 0, 0, ...</td>\n",
       "      <td>[122140, 77036, 156817, 76957, 68818, 155600, ...</td>\n",
       "      <td>39</td>\n",
       "      <td>[5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[13, 0, 0, 7, 5, 0, 1, 2, 1, 11, 0, 1, 0, 0, 1...</td>\n",
       "      <td>[168054, 87218, 35731, 1, 20475, 9039, 124834,...</td>\n",
       "      <td>27</td>\n",
       "      <td>[3.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 5.0, ...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[498, 13, 4, 218, 376, 3, 2, 33, 16, 591, 10, ...</td>\n",
       "      <td>[163193, 156352, 102888, 19914, 169438, 55772,...</td>\n",
       "      <td>1513</td>\n",
       "      <td>[5.0, 5.0, 5.0, 5.0, 4.0, 4.0, 5.0, 5.0, 5.0, ...</td>\n",
       "      <td>1513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[161, 1, 1, 86, 93, 0, 0, 11, 2, 141, 0, 16, 0...</td>\n",
       "      <td>[72857, 38652, 160427, 55772, 119999, 141777, ...</td>\n",
       "      <td>376</td>\n",
       "      <td>[5.0, 5.0, 5.0, 5.0, 4.0, 4.0, 5.0, 4.0, 5.0, ...</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   u                                         techniques  \\\n",
       "0  0  [8, 0, 0, 5, 6, 0, 0, 1, 0, 9, 1, 0, 0, 0, 1, ...   \n",
       "1  1  [11, 0, 0, 2, 12, 0, 0, 0, 0, 14, 5, 0, 0, 0, ...   \n",
       "2  2  [13, 0, 0, 7, 5, 0, 1, 2, 1, 11, 0, 1, 0, 0, 1...   \n",
       "3  3  [498, 13, 4, 218, 376, 3, 2, 33, 16, 591, 10, ...   \n",
       "4  4  [161, 1, 1, 86, 93, 0, 0, 11, 2, 141, 0, 16, 0...   \n",
       "\n",
       "                                               items  n_items  \\\n",
       "0  [1118, 27680, 32541, 137353, 16428, 28815, 658...       31   \n",
       "1  [122140, 77036, 156817, 76957, 68818, 155600, ...       39   \n",
       "2  [168054, 87218, 35731, 1, 20475, 9039, 124834,...       27   \n",
       "3  [163193, 156352, 102888, 19914, 169438, 55772,...     1513   \n",
       "4  [72857, 38652, 160427, 55772, 119999, 141777, ...      376   \n",
       "\n",
       "                                             ratings  n_ratings  \n",
       "0  [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 4.0, 4.0, ...         31  \n",
       "1  [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...         39  \n",
       "2  [3.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 5.0, ...         27  \n",
       "3  [5.0, 5.0, 5.0, 5.0, 4.0, 4.0, 5.0, 5.0, 5.0, ...       1513  \n",
       "4  [5.0, 5.0, 5.0, 5.0, 4.0, 4.0, 5.0, 4.0, 5.0, ...        376  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user = pd.read_csv(\"PP_users.csv\")\n",
    "user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>recipe_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38094</td>\n",
       "      <td>40893</td>\n",
       "      <td>4</td>\n",
       "      <td>Great with a salad. Cooked on top of stove for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1293707</td>\n",
       "      <td>40893</td>\n",
       "      <td>5</td>\n",
       "      <td>So simple, so delicious! Great for chilly fall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8937</td>\n",
       "      <td>44394</td>\n",
       "      <td>4</td>\n",
       "      <td>This worked very well and is EASY.  I used not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>126440</td>\n",
       "      <td>85009</td>\n",
       "      <td>5</td>\n",
       "      <td>I made the Mexican topping and took it to bunk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57222</td>\n",
       "      <td>85009</td>\n",
       "      <td>5</td>\n",
       "      <td>Made the cheddar bacon topping, adding a sprin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  recipe_id  rating  \\\n",
       "0    38094      40893       4   \n",
       "1  1293707      40893       5   \n",
       "2     8937      44394       4   \n",
       "3   126440      85009       5   \n",
       "4    57222      85009       5   \n",
       "\n",
       "                                              review  \n",
       "0  Great with a salad. Cooked on top of stove for...  \n",
       "1  So simple, so delicious! Great for chilly fall...  \n",
       "2  This worked very well and is EASY.  I used not...  \n",
       "3  I made the Mexican topping and took it to bunk...  \n",
       "4  Made the cheddar bacon topping, adding a sprin...  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.read_csv(\"RAW_interactions.csv\")\n",
    "reviews = reviews.drop(\"date\", axis=1)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1132367, 4)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_small = reviews.iloc[:100000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  user_idx  recipe_id  recipe_idx\n",
      "0    38094      1204      40893        2187\n",
      "1  1293707     24101      40893        2187\n",
      "2     8937       150      44394        2362\n",
      "3   126440      4629      85009        4609\n",
      "4    57222      2122      85009        4609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_12836\\2548817215.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reviews_small['user_idx'] = user_encoder.fit_transform(reviews_small['user_id'])\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_12836\\2548817215.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reviews_small['recipe_idx'] = recipe_encoder.fit_transform(reviews_small['recipe_id'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Map user_id and recipe_id to unique integers\n",
    "user_encoder = LabelEncoder()\n",
    "recipe_encoder = LabelEncoder()\n",
    "\n",
    "reviews_small['user_idx'] = user_encoder.fit_transform(reviews_small['user_id'])\n",
    "reviews_small['recipe_idx'] = recipe_encoder.fit_transform(reviews_small['recipe_id'])\n",
    "\n",
    "print(reviews_small[['user_id', 'user_idx', 'recipe_id', 'recipe_idx']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEXT EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "reviews_only = reviews_small[\"review\"].fillna(\"\").str.lower().str.replace(r\"[^\\w\\s]\", \"\", regex=True)\n",
    "\n",
    "sentences = reviews_only.apply(lambda x: str(x).split())\n",
    "\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "word_embeddings = {word: word2vec_model.wv[word] for word in word2vec_model.wv.index_to_key}\n",
    "\n",
    "def get_sentence_embedding(sentence):\n",
    "    words = sentence.split()\n",
    "    embeddings = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "\n",
    "sentence_embeddings = reviews_only.apply(get_sentence_embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPARING TRAIN, TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max user index in training: 32373, Unique users in training: 32374\n",
      "Max recipe index in training: 17861, Unique recipes in training: 17862\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_df = pd.DataFrame({\n",
    "    'user_id': reviews_small['user_id'],\n",
    "    'recipe_id': reviews_small['recipe_id'],\n",
    "    'review_embedding': list(sentence_embeddings),\n",
    "    'rating': reviews_small['rating']\n",
    "})\n",
    "\n",
    "train_df, test_df = train_test_split(data_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use Categorical to encode users and recipes, ensuring consistent mapping\n",
    "train_df['user_idx'] = pd.Categorical(train_df['user_id']).codes\n",
    "train_df['recipe_idx'] = pd.Categorical(train_df['recipe_id']).codes\n",
    "\n",
    "test_df['user_idx'] = pd.Categorical(\n",
    "    test_df['user_id'], categories=train_df['user_id'].unique()\n",
    ").codes\n",
    "\n",
    "test_df['recipe_idx'] = pd.Categorical(\n",
    "    test_df['recipe_id'], categories=train_df['recipe_id'].unique()\n",
    ").codes\n",
    "\n",
    "test_df['user_idx'] = test_df['user_idx'].replace(-1, len(train_df['user_id'].unique()))\n",
    "test_df['recipe_idx'] = test_df['recipe_idx'].replace(-1, len(train_df['recipe_id'].unique()))\n",
    "\n",
    "X_train_user_idx = train_df['user_idx'].values\n",
    "X_train_recipe_idx = train_df['recipe_idx'].values\n",
    "X_train_review_embedding = np.array(train_df['review_embedding'].tolist())\n",
    "y_train = train_df['rating'].values\n",
    "\n",
    "X_test_user_idx = test_df['user_idx'].values\n",
    "X_test_recipe_idx = test_df['recipe_idx'].values\n",
    "X_test_review_embedding = np.array(test_df['review_embedding'].tolist())\n",
    "y_test = test_df['rating'].values\n",
    "\n",
    "print(f\"Max user index in training: {max(X_train_user_idx)}, Unique users in training: {len(np.unique(X_train_user_idx))}\")\n",
    "print(f\"Max recipe index in training: {max(X_train_recipe_idx)}, Unique recipes in training: {len(np.unique(X_train_recipe_idx))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "user_idx_train_tensor = torch.tensor(X_train_user_idx, dtype=torch.long)\n",
    "recipe_idx_train_tensor = torch.tensor(X_train_recipe_idx, dtype=torch.long)\n",
    "review_embedding_train_tensor = torch.tensor(np.array(X_train_review_embedding.tolist()), dtype=torch.float32)\n",
    "rating_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "user_idx_test_tensor = torch.tensor(X_test_user_idx, dtype=torch.long)\n",
    "recipe_idx_test_tensor = torch.tensor(X_test_recipe_idx, dtype=torch.long)\n",
    "review_embedding_test_tensor = torch.tensor(np.array(X_test_review_embedding.tolist()), dtype=torch.float32)\n",
    "rating_test_tensor = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RatingPredictionModel(nn.Module):\n",
    "    def __init__(self, num_users, num_recipes, review_embedding_dim, embedding_dim, dropout_rate=0.1):\n",
    "        super(RatingPredictionModel, self).__init__()\n",
    "        \n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.recipe_embedding = nn.Embedding(num_recipes, embedding_dim)\n",
    "        \n",
    "        self.fc1 = nn.Linear(embedding_dim * 2 + review_embedding_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, user_idx, recipe_idx, review_embedding):\n",
    "        user_vec = self.user_embedding(user_idx)\n",
    "        recipe_vec = self.recipe_embedding(recipe_idx)\n",
    "        \n",
    "        x = torch.cat([user_vec, recipe_vec, review_embedding], dim=1)\n",
    "        \n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = len(user_encoder.classes_) + 1\n",
    "num_recipes = len(user_encoder.classes_) + 1\n",
    "review_embedding_dim = review_embedding_train_tensor.shape[1]\n",
    "embedding_dim = 30\n",
    "\n",
    "model = RatingPredictionModel(num_users, num_recipes, review_embedding_dim, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/170, Training MSE: 20.171756744384766\n",
      "Epoch 2/170, Training MSE: 19.44502830505371\n",
      "Epoch 3/170, Training MSE: 18.734329223632812\n",
      "Epoch 4/170, Training MSE: 18.02838706970215\n",
      "Epoch 5/170, Training MSE: 17.30881690979004\n",
      "Epoch 6/170, Training MSE: 16.5572566986084\n",
      "Epoch 7/170, Training MSE: 15.758737564086914\n",
      "Epoch 8/170, Training MSE: 14.902666091918945\n",
      "Epoch 9/170, Training MSE: 13.982306480407715\n",
      "Epoch 10/170, Training MSE: 12.995048522949219\n",
      "Epoch 11/170, Training MSE: 11.942282676696777\n",
      "Epoch 12/170, Training MSE: 10.829741477966309\n",
      "Epoch 13/170, Training MSE: 9.668360710144043\n",
      "Epoch 14/170, Training MSE: 8.474952697753906\n",
      "Epoch 15/170, Training MSE: 7.2736592292785645\n",
      "Epoch 16/170, Training MSE: 6.096723556518555\n",
      "Epoch 17/170, Training MSE: 4.985211372375488\n",
      "Epoch 18/170, Training MSE: 3.98888897895813\n",
      "Epoch 19/170, Training MSE: 3.163602352142334\n",
      "Epoch 20/170, Training MSE: 2.5606741905212402\n",
      "Epoch 21/170, Training MSE: 2.1968138217926025\n",
      "Epoch 22/170, Training MSE: 2.0207009315490723\n",
      "Epoch 23/170, Training MSE: 1.952012300491333\n",
      "Epoch 24/170, Training MSE: 1.9282808303833008\n",
      "Epoch 25/170, Training MSE: 1.9204823970794678\n",
      "Epoch 26/170, Training MSE: 1.9182027578353882\n",
      "Epoch 27/170, Training MSE: 1.9173587560653687\n",
      "Epoch 28/170, Training MSE: 1.917316198348999\n",
      "Epoch 29/170, Training MSE: 1.9174422025680542\n",
      "Epoch 30/170, Training MSE: 1.9173476696014404\n",
      "Epoch 31/170, Training MSE: 1.9171408414840698\n",
      "Epoch 32/170, Training MSE: 1.916673183441162\n",
      "Epoch 33/170, Training MSE: 1.9161624908447266\n",
      "Epoch 34/170, Training MSE: 1.9155824184417725\n",
      "Epoch 35/170, Training MSE: 1.9150521755218506\n",
      "Epoch 36/170, Training MSE: 1.9145727157592773\n",
      "Epoch 37/170, Training MSE: 1.9141172170639038\n",
      "Epoch 38/170, Training MSE: 1.9137353897094727\n",
      "Epoch 39/170, Training MSE: 1.9134193658828735\n",
      "Epoch 40/170, Training MSE: 1.91312575340271\n",
      "Epoch 41/170, Training MSE: 1.9128543138504028\n",
      "Epoch 42/170, Training MSE: 1.9126179218292236\n",
      "Epoch 43/170, Training MSE: 1.9123972654342651\n",
      "Epoch 44/170, Training MSE: 1.9121894836425781\n",
      "Epoch 45/170, Training MSE: 1.9119930267333984\n",
      "Epoch 46/170, Training MSE: 1.9117969274520874\n",
      "Epoch 47/170, Training MSE: 1.9116089344024658\n",
      "Epoch 48/170, Training MSE: 1.9114350080490112\n",
      "Epoch 49/170, Training MSE: 1.9112732410430908\n",
      "Epoch 50/170, Training MSE: 1.9111223220825195\n",
      "Epoch 51/170, Training MSE: 1.9109798669815063\n",
      "Epoch 52/170, Training MSE: 1.9108424186706543\n",
      "Epoch 53/170, Training MSE: 1.9107109308242798\n",
      "Epoch 54/170, Training MSE: 1.9105842113494873\n",
      "Epoch 55/170, Training MSE: 1.9104619026184082\n",
      "Epoch 56/170, Training MSE: 1.9103422164916992\n",
      "Epoch 57/170, Training MSE: 1.9102240800857544\n",
      "Epoch 58/170, Training MSE: 1.9101070165634155\n",
      "Epoch 59/170, Training MSE: 1.9099916219711304\n",
      "Epoch 60/170, Training MSE: 1.9098769426345825\n",
      "Epoch 61/170, Training MSE: 1.9097636938095093\n",
      "Epoch 62/170, Training MSE: 1.909651517868042\n",
      "Epoch 63/170, Training MSE: 1.9095406532287598\n",
      "Epoch 64/170, Training MSE: 1.9094302654266357\n",
      "Epoch 65/170, Training MSE: 1.9093207120895386\n",
      "Epoch 66/170, Training MSE: 1.909211277961731\n",
      "Epoch 67/170, Training MSE: 1.909102201461792\n",
      "Epoch 68/170, Training MSE: 1.9089933633804321\n",
      "Epoch 69/170, Training MSE: 1.9088845252990723\n",
      "Epoch 70/170, Training MSE: 1.9087761640548706\n",
      "Epoch 71/170, Training MSE: 1.9086679220199585\n",
      "Epoch 72/170, Training MSE: 1.9085601568222046\n",
      "Epoch 73/170, Training MSE: 1.9084523916244507\n",
      "Epoch 74/170, Training MSE: 1.9083448648452759\n",
      "Epoch 75/170, Training MSE: 1.9082374572753906\n",
      "Epoch 76/170, Training MSE: 1.9081305265426636\n",
      "Epoch 77/170, Training MSE: 1.9080229997634888\n",
      "Epoch 78/170, Training MSE: 1.907915472984314\n",
      "Epoch 79/170, Training MSE: 1.9078078269958496\n",
      "Epoch 80/170, Training MSE: 1.9076989889144897\n",
      "Epoch 81/170, Training MSE: 1.9075900316238403\n",
      "Epoch 82/170, Training MSE: 1.9074808359146118\n",
      "Epoch 83/170, Training MSE: 1.9073716402053833\n",
      "Epoch 84/170, Training MSE: 1.9072623252868652\n",
      "Epoch 85/170, Training MSE: 1.9071528911590576\n",
      "Epoch 86/170, Training MSE: 1.9070435762405396\n",
      "Epoch 87/170, Training MSE: 1.9069340229034424\n",
      "Epoch 88/170, Training MSE: 1.9068244695663452\n",
      "Epoch 89/170, Training MSE: 1.9067150354385376\n",
      "Epoch 90/170, Training MSE: 1.9066054821014404\n",
      "Epoch 91/170, Training MSE: 1.9064960479736328\n",
      "Epoch 92/170, Training MSE: 1.9063867330551147\n",
      "Epoch 93/170, Training MSE: 1.9062772989273071\n",
      "Epoch 94/170, Training MSE: 1.906167984008789\n",
      "Epoch 95/170, Training MSE: 1.9060585498809814\n",
      "Epoch 96/170, Training MSE: 1.9059478044509888\n",
      "Epoch 97/170, Training MSE: 1.9058369398117065\n",
      "Epoch 98/170, Training MSE: 1.9057247638702393\n",
      "Epoch 99/170, Training MSE: 1.9056123495101929\n",
      "Epoch 100/170, Training MSE: 1.9054982662200928\n",
      "Epoch 101/170, Training MSE: 1.9053839445114136\n",
      "Epoch 102/170, Training MSE: 1.9052693843841553\n",
      "Epoch 103/170, Training MSE: 1.9051544666290283\n",
      "Epoch 104/170, Training MSE: 1.9050394296646118\n",
      "Epoch 105/170, Training MSE: 1.9049228429794312\n",
      "Epoch 106/170, Training MSE: 1.9048056602478027\n",
      "Epoch 107/170, Training MSE: 1.904687523841858\n",
      "Epoch 108/170, Training MSE: 1.904569387435913\n",
      "Epoch 109/170, Training MSE: 1.90445077419281\n",
      "Epoch 110/170, Training MSE: 1.9043320417404175\n",
      "Epoch 111/170, Training MSE: 1.904213309288025\n",
      "Epoch 112/170, Training MSE: 1.9040945768356323\n",
      "Epoch 113/170, Training MSE: 1.9039753675460815\n",
      "Epoch 114/170, Training MSE: 1.9038563966751099\n",
      "Epoch 115/170, Training MSE: 1.9037364721298218\n",
      "Epoch 116/170, Training MSE: 1.9036160707473755\n",
      "Epoch 117/170, Training MSE: 1.9034953117370605\n",
      "Epoch 118/170, Training MSE: 1.903374195098877\n",
      "Epoch 119/170, Training MSE: 1.9032535552978516\n",
      "Epoch 120/170, Training MSE: 1.9031318426132202\n",
      "Epoch 121/170, Training MSE: 1.9030089378356934\n",
      "Epoch 122/170, Training MSE: 1.9028853178024292\n",
      "Epoch 123/170, Training MSE: 1.9027609825134277\n",
      "Epoch 124/170, Training MSE: 1.902635097503662\n",
      "Epoch 125/170, Training MSE: 1.90250825881958\n",
      "Epoch 126/170, Training MSE: 1.9023808240890503\n",
      "Epoch 127/170, Training MSE: 1.9022531509399414\n",
      "Epoch 128/170, Training MSE: 1.9021250009536743\n",
      "Epoch 129/170, Training MSE: 1.9019970893859863\n",
      "Epoch 130/170, Training MSE: 1.9018687009811401\n",
      "Epoch 131/170, Training MSE: 1.9017401933670044\n",
      "Epoch 132/170, Training MSE: 1.9016107320785522\n",
      "Epoch 133/170, Training MSE: 1.9014812707901\n",
      "Epoch 134/170, Training MSE: 1.901351809501648\n",
      "Epoch 135/170, Training MSE: 1.901221513748169\n",
      "Epoch 136/170, Training MSE: 1.9010908603668213\n",
      "Epoch 137/170, Training MSE: 1.9009599685668945\n",
      "Epoch 138/170, Training MSE: 1.9008290767669678\n",
      "Epoch 139/170, Training MSE: 1.9006980657577515\n",
      "Epoch 140/170, Training MSE: 1.9005674123764038\n",
      "Epoch 141/170, Training MSE: 1.900436282157898\n",
      "Epoch 142/170, Training MSE: 1.9003013372421265\n",
      "Epoch 143/170, Training MSE: 1.9001657962799072\n",
      "Epoch 144/170, Training MSE: 1.9000301361083984\n",
      "Epoch 145/170, Training MSE: 1.8998935222625732\n",
      "Epoch 146/170, Training MSE: 1.8997552394866943\n",
      "Epoch 147/170, Training MSE: 1.899613857269287\n",
      "Epoch 148/170, Training MSE: 1.8994678258895874\n",
      "Epoch 149/170, Training MSE: 1.899320363998413\n",
      "Epoch 150/170, Training MSE: 1.8991711139678955\n",
      "Epoch 151/170, Training MSE: 1.8990187644958496\n",
      "Epoch 152/170, Training MSE: 1.898863673210144\n",
      "Epoch 153/170, Training MSE: 1.898707628250122\n",
      "Epoch 154/170, Training MSE: 1.8985484838485718\n",
      "Epoch 155/170, Training MSE: 1.8983876705169678\n",
      "Epoch 156/170, Training MSE: 1.8982255458831787\n",
      "Epoch 157/170, Training MSE: 1.8980621099472046\n",
      "Epoch 158/170, Training MSE: 1.8978954553604126\n",
      "Epoch 159/170, Training MSE: 1.8977279663085938\n",
      "Epoch 160/170, Training MSE: 1.8975601196289062\n",
      "Epoch 161/170, Training MSE: 1.897390604019165\n",
      "Epoch 162/170, Training MSE: 1.897220253944397\n",
      "Epoch 163/170, Training MSE: 1.8970496654510498\n",
      "Epoch 164/170, Training MSE: 1.896878957748413\n",
      "Epoch 165/170, Training MSE: 1.8967046737670898\n",
      "Epoch 166/170, Training MSE: 1.8965272903442383\n",
      "Epoch 167/170, Training MSE: 1.8963472843170166\n",
      "Epoch 168/170, Training MSE: 1.8961632251739502\n",
      "Epoch 169/170, Training MSE: 1.8959767818450928\n",
      "Epoch 170/170, Training MSE: 1.8957895040512085\n"
     ]
    }
   ],
   "source": [
    "epochs = 170\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Ensure the model is in training mode\n",
    "    optimizer.zero_grad()  # Zero gradients before the backward pass\n",
    "    \n",
    "    # Forward pass: Get predictions using the training set\n",
    "    predictions_train = model(user_idx_train_tensor, recipe_idx_train_tensor, review_embedding_train_tensor)\n",
    "    \n",
    "    # Compute loss for the training set\n",
    "    loss = criterion(predictions_train.squeeze(), rating_train_tensor)\n",
    "    \n",
    "    # Backward pass: Compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update model weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print MSE after each epoch\n",
    "    mse = loss.item()  # MSE is the loss value (MSELoss returns a scalar)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training MSE: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.364762876585554\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    # Convert test inputs to tensors\n",
    "    user_idx_tensor = torch.tensor(X_test_user_idx, dtype=torch.long)\n",
    "    recipe_idx_tensor = torch.tensor(X_test_recipe_idx, dtype=torch.long)\n",
    "    review_embedding_tensor = torch.tensor(X_test_review_embedding, dtype=torch.float)\n",
    "\n",
    "    # Get predictions\n",
    "    y_pred = model(user_idx_tensor, recipe_idx_tensor, review_embedding_tensor)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with weight_decay=0.0\n",
      "Weight Decay: 0.0, Epoch: 1, Train Loss: 1.7441638114929199, Val Loss: 1.4882883456187508\n",
      "Weight Decay: 0.0, Epoch: 2, Train Loss: 1.4120799545884133, Val Loss: 1.4163859379938997\n",
      "Weight Decay: 0.0, Epoch: 3, Train Loss: 1.2776583253622056, Val Loss: 1.3950434180494315\n",
      "Weight Decay: 0.0, Epoch: 4, Train Loss: 1.163794349384308, Val Loss: 1.3873980592805357\n",
      "Weight Decay: 0.0, Epoch: 5, Train Loss: 1.0576284823894502, Val Loss: 1.4628228221457606\n",
      "Weight Decay: 0.0, Epoch: 6, Train Loss: 0.9301262266993523, Val Loss: 1.6144630573808956\n",
      "Weight Decay: 0.0, Epoch: 7, Train Loss: 0.7884988143324853, Val Loss: 1.6357947663186838\n",
      "Weight Decay: 0.0, Epoch: 8, Train Loss: 0.6510674505829811, Val Loss: 1.8393964933130307\n",
      "Weight Decay: 0.0, Epoch: 9, Train Loss: 0.5212625410020352, Val Loss: 1.8063482805943718\n",
      "Weight Decay: 0.0, Epoch: 10, Train Loss: 0.4145885623216629, Val Loss: 2.0397689938545227\n",
      "\n",
      "Training with weight_decay=0.1\n",
      "Weight Decay: 0.1, Epoch: 1, Train Loss: 1.7819724498033525, Val Loss: 1.4754854936759694\n",
      "Weight Decay: 0.1, Epoch: 2, Train Loss: 1.4790267540693283, Val Loss: 1.468386182008079\n",
      "Weight Decay: 0.1, Epoch: 3, Train Loss: 1.4644739502429962, Val Loss: 1.4140215202832755\n",
      "Weight Decay: 0.1, Epoch: 4, Train Loss: 1.4583420657634736, Val Loss: 1.4101874112321164\n",
      "Weight Decay: 0.1, Epoch: 5, Train Loss: 1.452683772277832, Val Loss: 1.4089355932447476\n",
      "Weight Decay: 0.1, Epoch: 6, Train Loss: 1.4481935766458511, Val Loss: 1.4537668243383828\n",
      "Weight Decay: 0.1, Epoch: 7, Train Loss: 1.44731343562603, Val Loss: 1.4084997506568226\n",
      "Weight Decay: 0.1, Epoch: 8, Train Loss: 1.4437583478927611, Val Loss: 1.4417991636279293\n",
      "Weight Decay: 0.1, Epoch: 9, Train Loss: 1.4360400625705718, Val Loss: 1.4150556964805712\n",
      "Weight Decay: 0.1, Epoch: 10, Train Loss: 1.4244594447374344, Val Loss: 1.3908485504575432\n",
      "\n",
      "Training with weight_decay=0.01\n",
      "Weight Decay: 0.01, Epoch: 1, Train Loss: 1.7547101940870284, Val Loss: 1.473348746951015\n",
      "Weight Decay: 0.01, Epoch: 2, Train Loss: 1.3906303512334823, Val Loss: 1.4295041288811559\n",
      "Weight Decay: 0.01, Epoch: 3, Train Loss: 1.3393782132863998, Val Loss: 1.2925960547246111\n",
      "Weight Decay: 0.01, Epoch: 4, Train Loss: 1.3175489487290382, Val Loss: 1.3152288400327055\n",
      "Weight Decay: 0.01, Epoch: 5, Train Loss: 1.3053983810424805, Val Loss: 1.3037440727313105\n",
      "Weight Decay: 0.01, Epoch: 6, Train Loss: 1.2956871670842172, Val Loss: 1.2659290193464048\n",
      "Weight Decay: 0.01, Epoch: 7, Train Loss: 1.2857380136489869, Val Loss: 1.2799833977755648\n",
      "Weight Decay: 0.01, Epoch: 8, Train Loss: 1.269083115708828, Val Loss: 1.2675448754629768\n",
      "Weight Decay: 0.01, Epoch: 9, Train Loss: 1.2564106972932816, Val Loss: 1.243861802469808\n",
      "Weight Decay: 0.01, Epoch: 10, Train Loss: 1.2376907262921333, Val Loss: 1.3052789313724626\n",
      "\n",
      "Training with weight_decay=0.001\n",
      "Weight Decay: 0.001, Epoch: 1, Train Loss: 1.6978475399017334, Val Loss: 1.43365870249538\n",
      "Weight Decay: 0.001, Epoch: 2, Train Loss: 1.4231350141763688, Val Loss: 1.3388964843254882\n",
      "Weight Decay: 0.001, Epoch: 3, Train Loss: 1.2864435426831244, Val Loss: 1.2596936759095603\n",
      "Weight Decay: 0.001, Epoch: 4, Train Loss: 1.1817236007928849, Val Loss: 1.4672152181022085\n",
      "Weight Decay: 0.001, Epoch: 5, Train Loss: 0.984314608180523, Val Loss: 1.375436269056302\n",
      "Weight Decay: 0.001, Epoch: 6, Train Loss: 0.6717995515525341, Val Loss: 1.7614336951662557\n",
      "Weight Decay: 0.001, Epoch: 7, Train Loss: 0.4922397370636463, Val Loss: 1.8567588741596515\n",
      "Weight Decay: 0.001, Epoch: 8, Train Loss: 0.45990804386138917, Val Loss: 1.957472970881782\n",
      "Weight Decay: 0.001, Epoch: 9, Train Loss: 0.43742821746468546, Val Loss: 1.8926340281582488\n",
      "Weight Decay: 0.001, Epoch: 10, Train Loss: 0.42982013701200483, Val Loss: 1.8815701095440898\n",
      "\n",
      "Training with weight_decay=0.0001\n",
      "Weight Decay: 0.0001, Epoch: 1, Train Loss: 1.697562068605423, Val Loss: 1.4278988569689255\n",
      "Weight Decay: 0.0001, Epoch: 2, Train Loss: 1.4302937306642531, Val Loss: 1.3925760894918595\n",
      "Weight Decay: 0.0001, Epoch: 3, Train Loss: 1.279554542016983, Val Loss: 1.3063323152141448\n",
      "Weight Decay: 0.0001, Epoch: 4, Train Loss: 1.0311267278671266, Val Loss: 1.5389954706731315\n",
      "Weight Decay: 0.0001, Epoch: 5, Train Loss: 0.6222955698132515, Val Loss: 1.955882507105605\n",
      "Weight Decay: 0.0001, Epoch: 6, Train Loss: 0.4639655354082584, Val Loss: 1.9072400624759662\n",
      "Weight Decay: 0.0001, Epoch: 7, Train Loss: 0.3942384346961975, Val Loss: 1.8950609265805813\n",
      "Weight Decay: 0.0001, Epoch: 8, Train Loss: 0.3593112329900265, Val Loss: 2.1720525049173034\n",
      "Weight Decay: 0.0001, Epoch: 9, Train Loss: 0.3457668087542057, Val Loss: 2.0672725976084747\n",
      "Weight Decay: 0.0001, Epoch: 10, Train Loss: 0.3263383006811142, Val Loss: 1.994513682664012\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(weight_decay, train_loader, val_loader, num_epochs=10):\n",
    "    # Initialize model\n",
    "    model = RatingPredictionModel(num_users, num_recipes, review_embedding_dim, embedding_dim)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=weight_decay)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for user_idx, recipe_idx, review_embedding, ratings in train_loader:            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(user_idx, recipe_idx, review_embedding)\n",
    "            loss = criterion(predictions.squeeze(), ratings)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for user_idx, recipe_idx, review_embedding, ratings in val_loader:\n",
    "                predictions = model(user_idx, recipe_idx, review_embedding)\n",
    "                val_loss += criterion(predictions.squeeze(), ratings).item()\n",
    "        \n",
    "        print(f\"Weight Decay: {weight_decay}, Epoch: {epoch+1}, \"\n",
    "              f\"Train Loss: {epoch_loss/len(train_loader)}, \"\n",
    "              f\"Val Loss: {val_loss/len(val_loader)}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define weight decay values to test\n",
    "weight_decay_values = [0.0, 0.1, 0.01, 0.001, 0.0001]\n",
    "\n",
    "# Prepare DataLoaders\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(user_idx_train_tensor, recipe_idx_train_tensor, review_embedding_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(user_idx_test_tensor, recipe_idx_test_tensor, review_embedding_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# Train models with different weight decay values\n",
    "trained_models = {}\n",
    "for wd in weight_decay_values:\n",
    "    print(f\"\\nTraining with weight_decay={wd}\")\n",
    "    trained_models[wd] = train_model(wd, train_loader, val_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
